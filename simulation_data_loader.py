"""
Simulation data loading and analysis utilities for TBG training data.

This module handles reading CSV files from training data generation,
processing simulation data, and providing structured data access for analysis.
Includes coordinate conversion utilities and data filtering capabilities.

Classes:
    simulation_data_analyzer: Main class for loading and analyzing simulation data

Functions:
    convert_relative_to_absolute_k: Convert k-points between coordinate systems
    get_k_space_unit_cell_limits: Get unit cell boundaries in k-space
    is_theta_equivalent: Check theta equivalence under symmetry
    load_simulation_data: Convenience function to load data from CSV
    quick_summary: Get quick statistics without full data loading
"""
import constants
from constants import csv, os
from constants import List, Dict, Optional, Union, Tuple
from Generate_training_data import compute_sym_factor
from data_structures_for_training_data import simulation_data_point

# Configure global logging
logger = constants.logging.getLogger(__name__)

def convert_relative_to_absolute_k(k_rel_x: float, k_rel_y: float, a: int, N_scale: float) -> Tuple[float, float]:
    """
    Convert k-points from relative coordinates to absolute coordinates.
    
    Args:
        k_rel_x: k-point x-coordinate in relative coordinates
        k_rel_y: k-point y-coordinate in relative coordinates  
        a: Integer twist parameter a
        N_scale: Scaling factor N
        
    Returns:
        Tuple of (k_x_abs, k_y_abs) in absolute coordinates
    """
    # Calculate dual vectors based on the same logic as TBG.py
    # The dual vectors are scaled by 1/N_scale
    N = N_scale
    
    if a % 3 == 0:
        # Use v1, v2 scaled by 1/N
        dual_vectors = [
            (constants.safe_divide(constants.v1[0], N, 0.0), constants.safe_divide(constants.v1[1], N, 0.0)),
            (constants.safe_divide(constants.v2[0], N, 0.0), constants.safe_divide(constants.v2[1], N, 0.0))
        ]
    else:
        # Use k1, k2 scaled by 1/N  
        dual_vectors = [
            (constants.safe_divide(constants.k1[0], N, 0.0), constants.safe_divide(constants.k1[1], N, 0.0)),
            (constants.safe_divide(constants.k2[0], N, 0.0), constants.safe_divide(constants.k2[1], N, 0.0))
        ]
    
    # Convert to absolute coordinates using the same formula as plotting.py
    k_x_abs = k_rel_x * dual_vectors[0][0] + k_rel_y * dual_vectors[1][0]
    k_y_abs = k_rel_x * dual_vectors[0][1] + k_rel_y * dual_vectors[1][1]
    
    return k_x_abs, k_y_abs

def get_k_space_unit_cell_limits() -> Tuple[Tuple[float, float], Tuple[float, float]]:
    """
    Get the k-space limits for the unscaled (N=1) unit cell.
    
    The unit cell in relative coordinates is centered and goes from -0.5 to 0.5 in both directions.
    
    Returns:
        Tuple of ((k_x_min, k_x_max), (k_y_min, k_y_max)) for the unit cell boundaries
    """
    # For N=1, the dual vectors are just the original constants.k1, constants.k2
    # The unit cell in relative coordinates spans from -0.5 to 0.5
    # So we need corners at: (-0.5,-0.5), (0.5,-0.5), (0.5,0.5), (-0.5,0.5)
    
    k1 = constants.k1
    k2 = constants.k2
    
    # Calculate the corners of the centered unit cell in absolute coordinates
    # Convert relative coordinates [-0.5, 0.5] range to absolute coordinates
    rel_coords = [(-0.5, -0.5), (0.5, -0.5), (0.5, 0.5), (-0.5, 0.5)]
    
    corners_x = []
    corners_y = []
    
    for rel_x, rel_y in rel_coords:
        # Convert relative to absolute using k1, k2 basis (with N=1)
        abs_x = rel_x * k1[0] + rel_y * k2[0]
        abs_y = rel_x * k1[1] + rel_y * k2[1]
        corners_x.append(abs_x)
        corners_y.append(abs_y)
    
    # Get the bounding box
    k_x_min, k_x_max = min(corners_x), max(corners_x)
    k_y_min, k_y_max = min(corners_y), max(corners_y)
    
    return (k_x_min, k_x_max), (k_y_min, k_y_max)

def is_theta_equivalent(theta1: float, theta2: float, tolerance: float = 1e-6) -> bool:
    """
    Check if two theta values are equivalent considering 60-degree symmetry.
    
    Args:
        theta1: First theta value in degrees
        theta2: Second theta value in degrees
        tolerance: Numerical tolerance for comparison
        
    Returns:
        True if the theta values are equivalent under 60-degree symmetry
    """
    # Normalize both angles to [0, 60) range
    normalized_theta1 = theta1 % 60
    normalized_theta2 = theta2 % 60
    
    # Check direct equivalence
    return abs(normalized_theta1 - normalized_theta2) < tolerance

class simulation_data_analyzer:
    """
    Main class for analyzing TBG simulation data.
    
    This class handles loading, processing, and analyzing simulation results
    from CSV files generated by the training data generation process.
    """
    
    def __init__(self, data_folder: str = None):
        """
        Initialize the analyzer.
        
        Args:
            data_folder (str, optional): Path to folder containing CSV data files.
                                       Defaults to constants.PATH + "/Data_from_run"
        """
        if data_folder is None:
            self.data_folder = os.path.join(constants.PATH, "Data_from_run")
        else:
            self.data_folder = data_folder
            
        self.data_points: List[simulation_data_point] = []
        self.raw_data: List[Dict[str, str]] = []
        self.loaded_files: List[str] = []  # Track loaded file names
        self.file_colors: Dict[str, str] = {}  # Map file names to colors
        
        logger.info(f"Initialized simulation_data_analyzer with data folder: {self.data_folder}")

    def load_csv_data(self, filename: str = "dirac_training_data.csv") -> None:
        """
        Load simulation data from CSV file.
        
        Args:
            filename (str): Name of the CSV file to load
            
        Raises:
            FileNotFoundError: If the specified file doesn't exist
            ValueError: If the CSV file has incorrect format
        """
        # Clear existing data before loading single file
        self.data_points = []
        self.raw_data = []
        self.loaded_files = []
        self.file_colors = {}
        
        self.load_multiple_csv_files([filename])
    
    def load_multiple_csv_files(self, filenames: List[str], clear_existing: bool = True, remove_duplicates: bool = True) -> None:
        """
        Load simulation data from multiple CSV files.
        
        Args:
            filenames (List[str]): List of CSV file names to load
            clear_existing (bool): Whether to clear existing data before loading
            remove_duplicates (bool): Whether to remove duplicate points after loading (default True)
            
        Raises:
            FileNotFoundError: If any specified file doesn't exist
            ValueError: If any CSV file has incorrect format
        """
        if clear_existing:
            self.data_points = []
            self.raw_data = []
            self.loaded_files = []
            self.file_colors = {}

        
        # Use a combination of distinct color palettes for better separation
        distinct_colors = [
            '#1f77b4',  # Blue
            '#ff7f0e',  # Orange  
            '#2ca02c',  # Green
            '#d62728',  # Red
            '#9467bd',  # Purple
            '#8c564b',  # Brown
            '#e377c2',  # Pink
            '#7f7f7f',  # Gray
            '#bcbd22',  # Olive
            '#17becf',  # Cyan
            '#aec7e8',  # Light Blue
            '#ffbb78',  # Light Orange
            '#98df8a',  # Light Green
            '#ff9896',  # Light Red
            '#c5b0d5',  # Light Purple
        ]
        
        for file_index, filename in enumerate(filenames):
            filepath = os.path.join(self.data_folder, filename)
            
            if not os.path.exists(filepath):
                raise FileNotFoundError(f"Data file not found: {filepath}")
            
            try:
                # Read the CSV file
                with open(filepath, 'r', newline='') as csvfile:
                    reader = csv.DictReader(csvfile)
                    file_raw_data = list(reader)
                
                # Add source file info to each row
                for row in file_raw_data:
                    row['_source_file'] = filename
                    row['_file_index'] = file_index
                
                self.raw_data.extend(file_raw_data)
                self.loaded_files.append(filename)
                
                # Assign distinct color to this file
                # Calculate the actual index considering already loaded files
                actual_file_index = len(self.loaded_files) - 1
                color_index = actual_file_index % len(distinct_colors)
                self.file_colors[filename] = distinct_colors[color_index]
                
                logger.info(f"Loaded {len(file_raw_data)} rows from {filepath}")
                
                # Validate required columns for first file
                if file_index == 0:
                    required_columns = [
                        'a', 'b', 'interlayer_dist_threshold', 'intralayer_dist_threshold',
                        'inter_graph_weight', 'intra_graph_weight', 'target_k_x', 'target_k_y', 'Dirac_velocity'
                    ]
                    
                    if file_raw_data:
                        available_columns = set(file_raw_data[0].keys())
                        logger.info(f"Available columns: {sorted(available_columns)}")
                        missing_columns = [col for col in required_columns if col not in available_columns]
                        if missing_columns:
                            raise ValueError(f"Missing required columns in {filename}: {missing_columns}. Available: {sorted(available_columns)}")
                
            except Exception as e:
                logger.error(f"Error loading CSV data from {filename}: {e}")
                raise
        
        logger.info(f"Loaded {len(filenames)} files with {len(self.raw_data)} total rows")
        
        # Use full dataset for production training
        
        # Process the data
        self._process_raw_data()
        
        # Remove duplicates only if requested
        if remove_duplicates:
            self._remove_duplicates()
            logger.info("Duplicates removed for analysis")
        else:
            logger.info("Duplicates preserved for training")
    
    def _process_raw_data(self) -> None:
        """
        Process raw DataFrame into simulation_data_point objects.
        
        Calculates derived parameters (theta, weight_ratio) and creates
        structured data points for analysis.
        """
        self.data_points = []
        
        for row in self.raw_data:
            try:
                # Calculate theta = atan(sqrt(3)*b/a) in degrees
                a_val = int(row['a'])
                b_val = int(row['b'])
                theta = constants.np.arctan(constants.np.sqrt(3) * b_val / a_val) * 180 / constants.np.pi
                theta = min(theta, 60-theta) # we chose the smaller angle from the symmetry.

                # Calculate weight_ratio = intra_graph_weight/inter_graph_weight
                intra_weight = float(row['intra_graph_weight'])
                inter_weight = float(row['inter_graph_weight'])
                weight_ratio = intra_weight / inter_weight
                
                # Get k-point coordinates
                k_x_rel = float(row['target_k_x'])
                k_y_rel = float(row['target_k_y'])
                
                # Convert relative k-coordinates to absolute coordinates
                n_scale_val = float(row['N_scale']) if 'N_scale' in row and row['N_scale'].strip() else 1.0
                k_x_abs, k_y_abs = convert_relative_to_absolute_k(k_x_rel, k_y_rel, a_val, n_scale_val)
                
                # Get source file information
                source_file = row.get('_source_file', None)
                file_index = int(row.get('_file_index', 0)) if row.get('_file_index') is not None else None
                
                # Create data point
                data_point = simulation_data_point(
                    a=a_val,
                    b=b_val,
                    theta=theta,
                    interlayer_dist_threshold=float(row['interlayer_dist_threshold']),
                    intralayer_dist_threshold=float(row['intralayer_dist_threshold']),
                    inter_graph_weight=inter_weight,
                    intra_graph_weight=intra_weight,
                    weight_ratio=weight_ratio,
                    k_x=k_x_rel,
                    k_y=k_y_rel,
                    k_x_abs=k_x_abs,
                    k_y_abs=k_y_abs,
                    velocity=float(row['Dirac_velocity']),
                    n_scale=n_scale_val,
                    num_nodes=int(row['num_nodes']) if 'num_nodes' in row and row['num_nodes'].strip() else None,
                    source_file=source_file,
                    file_index=file_index
                )
                
                self.data_points.append(data_point)
                
            except Exception as e:
                logger.warning(f"Error processing row {len(self.data_points)}: {e}")
                continue
        
        logger.info(f"Processed {len(self.data_points)} valid data points")

    def _remove_duplicates(self) -> None:
        """
        Remove duplicate data points based on theta equivalence and symmetric cases.
        
        This removes duplicates WITHIN each file only, preserving data from different files
        even if they have the same parameters.
        
        This removes:
        1. Points with equivalent theta values (different a,b giving same theta) within same file
        2. Symmetric cases (where compute_sym_factor gives equivalent systems) within same file
        """
        if not self.data_points:
            return
            
        unique_points = []
        # Group points by source file and remove duplicates within each file
        file_groups = {}
        
        for point in self.data_points:
            file_key = point.source_file or 'default'
            if file_key not in file_groups:
                file_groups[file_key] = []
            file_groups[file_key].append(point)
        
        total_removed = 0
        
        # Process each file separately
        for file_key, file_points in file_groups.items():
            seen_configs = set()
            file_unique_points = []
            
            for point in file_points:
                # Create a configuration key for deduplication within this file
                # Use frozen parameters that define the physical system
                config_key = (
                    round(point.theta, 8),  # Round theta to avoid floating point issues
                    round(point.interlayer_dist_threshold, 6),
                    round(point.intralayer_dist_threshold, 6), 
                    round(point.inter_graph_weight, 6),
                    round(point.intra_graph_weight, 6)
                )
                
                # Check if we've seen this configuration in this file
                if config_key in seen_configs:
                    total_removed += 1
                    continue
                    
                # Check if symmetric case is already seen in this file
                try:
                    sym_a, sym_b = compute_sym_factor(point.a, point.b)
                    sym_theta = constants.np.arctan(constants.np.sqrt(3) * sym_b / sym_a) * 180 / constants.np.pi
                    sym_config_key = (
                        round(sym_theta, 8),
                        round(point.interlayer_dist_threshold, 6),
                        round(point.intralayer_dist_threshold, 6),
                        round(point.inter_graph_weight, 6),
                        round(point.intra_graph_weight, 6)
                    )
                    
                    if sym_config_key in seen_configs:
                        total_removed += 1
                        continue
                        
                except Exception as e:
                    logger.warning(f"Error checking symmetric case for a={point.a}, b={point.b}: {e}")
                
                # Add both configurations to seen set for this file
                seen_configs.add(config_key)
                if 'sym_config_key' in locals():
                    seen_configs.add(sym_config_key)
                    
                file_unique_points.append(point)
            
            unique_points.extend(file_unique_points)
            logger.info(f"File {file_key}: Removed {len(file_points) - len(file_unique_points)} duplicate points, "
                       f"kept {len(file_unique_points)} unique points")
        
        original_count = len(self.data_points)
        self.data_points = unique_points
        logger.info(f"Total removed {total_removed} duplicate points across all files, "
                   f"kept {len(unique_points)} unique points")

    def get_filtered_data(self, 
                         theta_range: Optional[Tuple[float, float]] = None,
                         weight_ratio_range: Optional[Tuple[float, float]] = None,
                         interlayer_threshold_range: Optional[Tuple[float, float]] = None,
                         intralayer_threshold_range: Optional[Tuple[float, float]] = None,
                         specific_theta: Optional[float] = None,
                         specific_weight_ratio: Optional[float] = None,
                         specific_interlayer_threshold: Optional[float] = None,
                         specific_intralayer_threshold: Optional[float] = None,
                         precision: int = 3) -> List[simulation_data_point]:
        """
        Get filtered simulation data based on parameter ranges or specific values.
        
        Args:
            theta_range: (min, max) range for theta filtering
            weight_ratio_range: (min, max) range for weight ratio filtering
            interlayer_threshold_range: (min, max) range for interlayer threshold filtering
            intralayer_threshold_range: (min, max) range for intralayer threshold filtering
            specific_theta: Specific theta value to match (with precision)
            specific_weight_ratio: Specific weight ratio to match (with precision)
            specific_interlayer_threshold: Specific interlayer threshold to match (with precision)
            specific_intralayer_threshold: Specific intralayer threshold to match (with precision)
            precision: Number of decimal places for float comparison
        
        Returns:
            List of filtered simulation_data_point objects
        """
        filtered_data = []
        
        for point in self.data_points:
            # Check theta filtering
            if theta_range is not None:
                if not (theta_range[0] <= point.theta <= theta_range[1]):
                    continue
            
            if specific_theta is not None:
                if abs(round(point.theta, precision) - round(specific_theta, precision)) > 1e-10:
                    continue
            
            # Check weight ratio filtering
            if weight_ratio_range is not None:
                if not (weight_ratio_range[0] <= point.weight_ratio <= weight_ratio_range[1]):
                    continue
                    
            if specific_weight_ratio is not None:
                if abs(round(point.weight_ratio, precision) - round(specific_weight_ratio, precision)) > 1e-10:
                    continue
            
            # Check interlayer threshold filtering
            if interlayer_threshold_range is not None:
                if not (interlayer_threshold_range[0] <= point.interlayer_dist_threshold <= interlayer_threshold_range[1]):
                    continue
                    
            if specific_interlayer_threshold is not None:
                if abs(round(point.interlayer_dist_threshold, precision) - round(specific_interlayer_threshold, precision)) > 1e-10:
                    continue
                    
            # Check intralayer threshold filtering
            if intralayer_threshold_range is not None:
                if not (intralayer_threshold_range[0] <= point.intralayer_dist_threshold <= intralayer_threshold_range[1]):
                    continue
                    
            if specific_intralayer_threshold is not None:
                if abs(round(point.intralayer_dist_threshold, precision) - round(specific_intralayer_threshold, precision)) > 1e-10:
                    continue
            
            filtered_data.append(point)
        
        return filtered_data

    def get_unique_parameter_values(self, parameter: str, precision: int = 3) -> List[Union[int, float]]:
        """
        Get sorted list of unique values for a parameter.
        
        Args:
            parameter: Parameter name ('theta', 'weight_ratio', etc.)
            precision: Number of decimal places for rounding float parameters
            
        Returns:
            Sorted list of unique parameter values
        """
        values = []
        for point in self.data_points:
            value = getattr(point, parameter)
            if isinstance(value, float):
                value = round(value, precision)
            values.append(value)
        
        return sorted(list(set(values)))

    def get_summary_statistics(self) -> Dict:
        """Get summary statistics for the loaded data."""
        if not self.data_points:
            return {}
        
        theta_values = [p.theta for p in self.data_points]
        weight_ratios = [p.weight_ratio for p in self.data_points]
        velocities = [p.velocity for p in self.data_points]
        
        return {
            'total_points': len(self.data_points),
            'theta_range': (min(theta_values), max(theta_values)),
            'theta_mean': constants.np.mean(theta_values),
            'velocity_range': (min(velocities), max(velocities)),
            'velocity_mean': constants.np.mean(velocities),
            'weight_ratio_range': (min(weight_ratios), max(weight_ratios)),
            'weight_ratio_mean': constants.np.mean(weight_ratios),
            'unique_a_values': len(set(dp.a for dp in self.data_points)),
            'unique_b_values': len(set(dp.b for dp in self.data_points))
        }
    
    def get_data_summary(self) -> Dict[str, Union[int, float, str]]:
        """
        Get summary statistics of the loaded data.
        
        Returns:
            Dict with summary statistics including counts, ranges, and basic stats
        """
        if not self.data_points:
            return {"error": "No data loaded"}
        
        return self.get_summary_statistics()
    
    def filter_data(self, **kwargs) -> List[simulation_data_point]:
        """
        Filter data points based on parameter ranges.
        
        Args:
            **kwargs: Parameter ranges in format param_min, param_max
                     e.g., theta_min=0.1, theta_max=0.5
        
        Returns:
            List of filtered simulation_data_point objects
        """
        filtered_points = []
        
        for dp in self.data_points:
            include = True
            
            # Check each filter condition
            for key, value in kwargs.items():
                if key.endswith('_min'):
                    param = key[:-4]  # Remove '_min'
                    if hasattr(dp, param) and getattr(dp, param) < value:
                        include = False
                        break
                elif key.endswith('_max'):
                    param = key[:-4]  # Remove '_max'
                    if hasattr(dp, param) and getattr(dp, param) > value:
                        include = False
                        break
            
            if include:
                filtered_points.append(dp)
        
        logger.info(f"Filtered to {len(filtered_points)} points from {len(self.data_points)} total")
        return filtered_points
    
    def get_parameter_arrays(self, parameters: List[str]) -> Dict[str, constants.np.ndarray]:
        """
        Extract parameter arrays for analysis or plotting.
        
        Args:
            parameters: List of parameter names to extract
            
        Returns:
            Dictionary mapping parameter names to numpy arrays
        """
        arrays = {}
        
        for param in parameters:
            if self.data_points and hasattr(self.data_points[0], param):
                values = [getattr(dp, param) for dp in self.data_points if getattr(dp, param) is not None]
                arrays[param] = constants.np.array(values)
            else:
                logger.warning(f"Parameter '{param}' not found in data structure")
        
        return arrays

def load_simulation_data(data_folder: str = None, filename: str = "dirac_training_data.csv") -> simulation_data_analyzer:
    """
    Load simulation data from CSV file.
    
    Args:
        data_folder: Path to data folder (defaults to constants.PATH)
        filename: Name of CSV file to load
        
    Returns:
        simulation_data_analyzer instance with loaded data
    """
    if data_folder is None:
        data_folder = constants.PATH
    
    filepath = os.path.join(data_folder, filename)
    return simulation_data_analyzer(filepath)

def quick_summary(data_folder: str = None, filename: str = "dirac_training_data.csv") -> Dict:
    """
    Get quick summary of simulation data without full loading.
    
    Args:
        data_folder: Path to data folder (defaults to constants.PATH)
        filename: Name of CSV file to analyze
        
    Returns:
        Dictionary with summary statistics
    """
    analyzer = load_simulation_data(data_folder, filename)
    return analyzer.get_summary_statistics()